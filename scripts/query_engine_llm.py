from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
from pathlib import Path
from ollama import Client
import tiktoken

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
INDEX_PATH = Path("vector_store/faiss.index")
META_PATH = Path("vector_store/metadata.pkl")
MAX_TOKENS = 1500
ollama = Client(host="http://localhost:11434")


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞
embedder = SentenceTransformer('intfloat/multilingual-e5-base')
index = faiss.read_index(str(INDEX_PATH))
enc = tiktoken.get_encoding("cl100k_base")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
with META_PATH.open("rb") as f:
    metadata = pickle.load(f)

# –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
def count_tokens(text: str) -> int:
    return len(enc.encode(text))

# –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
def get_confidence(filtered: list[dict]) -> str:
    min_dist = min(r["distance"] for r in filtered)
    if min_dist < 0.35:
        return "üü¢ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –≤—ã—Å–æ–∫–∞—è"
    elif min_dist < 0.55:
        return "üü° –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: —Å—Ä–µ–¥–Ω—è—è"
    else:
        return "üî¥ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –Ω–∏–∑–∫–∞—è"

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
def build_context(filtered: list[dict]) -> tuple[str, list[dict]]:
    context_chunks = []
    token_total = 0
    used = []

    for r in filtered:
        tokens = count_tokens(r["text"])
        if token_total + tokens > MAX_TOKENS:
            break
        context_chunks.append(r["text"])
        token_total += tokens
        used.append({
            "page": r["page"],
            "id": r["id"],
            "source": r["source"]
        })


    return "\n---\n".join(context_chunks), used


# –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
def search(query: str, top_k: int = 5) -> list[dict]:
    query_vec = embedder.encode(["query: " + query]).astype("float32")
    distances, ids = index.search(query_vec, top_k)

    results = []
    for idx, dist in zip(ids[0], distances[0]):
        meta = metadata[idx]
        results.append({
            "id": idx,
            "distance": float(dist),
            "text": meta.get("text", ""),
            "source": meta.get("source", ""),
            "page": meta.get("page", "?")
        })
    return results

# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
def search_with_llm(query: str, top_k: int = 5):
    results = search(query, top_k)
    filtered = [r for r in results if r["distance"] < 0.5]

    if not filtered:
        print("üî¥ –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Ç–≤–µ—Ç –Ω–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω.")
        return

    confidence = get_confidence(filtered)
    context, used_chunks = build_context(filtered)

    system = "–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –ø–æ –¥–µ–ª—É, –∫—Ä–∞—Ç–∫–æ, –±–µ–∑ –≤—ã–¥—É–º–æ–∫. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–û—Ç–≤–µ—Ç:"

    try:
        response = ollama.chat(
            model="mistral",
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": prompt}
            ]
        )
        print(f"\nüîç –û—Ç–≤–µ—Ç ({confidence}):\n" + response['message']['content'])
        print("\nüìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for s in used_chunks:
            print(f"- {s['source']} —Å—Ç—Ä. {s['page']} (id={s['id']})")

    except Exception as e:
        print("[‚úó] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama:")
        print(repr(e))
        print("\n[‚úé] –ü–æ—Å–ª–µ–¥–Ω–∏–π prompt –±—ã–ª:")
        print(prompt[:500])

if __name__ == "__main__":
    query = input("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å: ")
    search_with_llm(query)
